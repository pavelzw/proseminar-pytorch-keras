{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In diesem Beispiel sehen wir uns MNIST an.\n",
    "Hierbei handelt es sich um eine Datenbank aus\n",
    "70000 handgeschriebenen Ziffern, davon 60000 im Trainings-\n",
    "und 10000 im Testdatensatz."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_set = datasets.MNIST(\"data\", train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_set = datasets.MNIST(\"data\", train=False, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Diesen Datensatz packen wir nun in einen passenden `DataLoader`.\n",
    "Mit diesem können wir gut über Datensätze iterieren."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Das erste Element des DataLoaders sieht wie folgt aus:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([1, 5, 7, 1, 8, 9, 7, 1, 8, 9, 1, 8, 8, 1, 0, 3, 7, 6, 2, 5, 3, 1, 0, 0,\n",
      "        6, 8, 0, 2, 3, 4, 7, 8, 6, 3, 5, 0, 7, 6, 1, 9, 5, 9, 6, 3, 5, 7, 7, 3,\n",
      "        9, 0, 7, 7, 1, 9, 0, 0, 7, 9, 2, 2, 6, 3, 3, 7, 6, 2, 5, 2, 0, 3, 1, 7,\n",
      "        8, 7, 4, 1, 9, 5, 6, 7, 4, 1, 3, 8, 4, 6, 1, 1, 4, 7, 0, 5, 1, 9, 3, 3,\n",
      "        6, 1, 4, 1])]\n"
     ]
    }
   ],
   "source": [
    "fst_data = next(iter(train_loader))\n",
    "print(fst_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Betrachten beispielhaft wir die erste Ziffer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'Value: 1')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fst_image = fst_data[0][0]\n",
    "fst_digit = fst_data[1][0].item()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(fst_image.view(fst_image.shape[1], fst_image.shape[2]))\n",
    "plt.title(\"Value: \" + str(fst_digit))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistische Regression\n",
    "Nun erstellen wir die Architektur der logistischen Regression."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "n_iters = 3000\n",
    "epochs = int(n_iters / (len(train_set) / batch_size))\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "lr_rate = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_dim, output_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jetzt initialisieren wir die Loss class und den Optimizer (Stochastic Gradient Descent)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. Loss: 2.3342204093933105. Accuracy: 6.76.\n",
      "Iteration: 500. Loss: 1.879074215888977. Accuracy: 67.72.\n",
      "Iteration: 1000. Loss: 1.528935432434082. Accuracy: 76.71.\n",
      "Iteration: 1500. Loss: 1.314855933189392. Accuracy: 79.44.\n",
      "Iteration: 2000. Loss: 1.099409818649292. Accuracy: 80.97.\n",
      "Iteration: 2500. Loss: 1.0250781774520874. Accuracy: 82.07.\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(batch_size, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration = epoch * len(train_loader) + i\n",
    "        if iteration % 500 == 0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(batch_size, 28*28))\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "            accuracy = 100 * correct.item() / total\n",
    "            print(f\"Iteration: {iteration}. Loss: {loss.item()}. Accuracy: {accuracy}.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}