{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIXGeq5TMbvg",
    "outputId": "5e77de6e-3a3b-4cf5-b69c-7ca314292810",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\pavel\\anaconda3\\envs\\pytorch\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "import wget\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten laden und präparieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Av1RnZBMPfYn"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data/PetImages\"):\n",
    "    wget.download(\"https://oshi.at/twJYYz\", bar=wget.bar_adaptive)\n",
    "\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(\"JuIO.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"\")\n",
    "\n",
    "\n",
    "train_path = 'data/PetImages/train'\n",
    "valid_path = 'data/PetImages/valid'\n",
    "test_path = 'data/PetImages/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0aoL2ygUTUYD",
    "outputId": "25d7ed49-e207-431f-b883-78cf1c8712d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n",
      "Found 100 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "batch_size = 10\n",
    "\n",
    "classes = ['Cat', 'Dog']\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=train_path,\n",
    "                         target_size=(image_size, image_size),\n",
    "                         classes=classes,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=valid_path,\n",
    "                         target_size=(image_size, image_size),\n",
    "                         classes=classes,\n",
    "                         batch_size=batch_size)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=test_path,\n",
    "                         target_size=(image_size, image_size),\n",
    "                         classes=classes,\n",
    "                         batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronales Netz erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Definition\n",
    "Wir betrachten ein _unterschiedliches_, aber ähnliches Problem. Wenn hierzu bereits ein neuronales Netz besteht, welches dieses gut löst, können wir das bereits bestehende neuronale Netz verwenden und leicht abändern und ein paar Ebenen des Netzes neu trainieren, um es auf unser Problem zu spezifizieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow bietet in seiner Bibliothek bereits viele verschiedene vortrainierte Netze an. \n",
    "Diese kann man sich unter [https://www.tensorflow.org/api_docs/python/tf/keras/applications?hl=de](https://www.tensorflow.org/api_docs/python/tf/keras/applications?hl=de) ansehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "fLDYXbtfMbvi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wir verwenden nun das VGG16 Modell für Transfer Learning.\n",
    "Unser Ziel ist es, ein neuronales Netz zu erstellen, welches Bilder von Katzen und Hunden unterscheiden kann.\n",
    "Wie gerade gesehen, funktioniert dies mit einfachen CNNs nicht so gut.\n",
    "Deshalb versuchen wir es mit Transfer Learning bei einem bereits trainierten neuronalen Netz,\n",
    "welches wir auf unsere Bedürfnisse spezialisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJiO8nIYMbvk",
    "outputId": "ccc18b9f-6eee-43df-a833-d00c7f12bc82",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model = tf.keras.applications.vgg16.VGG16()\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5qpC_sYMqE6"
   },
   "source": [
    "Wir erstellen nun ein eigenes Modell, welches die gleichen Ebenen wie VGG16 hat außer der letzten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "56dS1sOGNLoM"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "for layer in vgg16_model.layers[:-1]:\n",
    "    model.add(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5HwiG-GN0OL"
   },
   "source": [
    "Da wir die Ebenen vor unserer Spezialisierung (dem letzten voll vernetzen neuronalen Netz) nicht erneut trainieren wollen,\n",
    "werden wir diese freezen, d. h. `trainable = False` setzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsGgTCN7Nj0V"
   },
   "source": [
    "Statt dem Dense Layer mit 1000 Outputs verwenden wir ein Dense Layer mit 2 Outputs - Katze oder Hund."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-L5qnw48OFBS"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Laxoy0tHNjbi",
    "outputId": "d57572a3-6f8e-47d1-b7c8-acb59350cc30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 8,194\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units=2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronales Netz trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9J5HPEpOovd"
   },
   "source": [
    "Nun haben wir ein Modell, welches nur 8194 anpassbare Parameter hat statt 134 Millionen.\n",
    "Wir verwenden wieder `Adam` als Optimizer und categorical crossentropy loss als loss funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IAtzv8dKO-AU"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwX__iQBUFWe",
    "outputId": "50a392c3-2b59-4e91-a5ff-b96ee4c802be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 - 11s - loss: 0.3470 - accuracy: 0.8560 - val_loss: 0.1996 - val_accuracy: 0.9350\n",
      "Epoch 2/3\n",
      "100/100 - 5s - loss: 0.0841 - accuracy: 0.9700 - val_loss: 0.1442 - val_accuracy: 0.9500\n",
      "Epoch 3/3\n",
      "100/100 - 5s - loss: 0.0551 - accuracy: 0.9800 - val_loss: 0.1414 - val_accuracy: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22c6889cc70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_batches, validation_data=valid_batches, epochs=3, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorhersagen auf Testdatensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtmIUk3Ryej8"
   },
   "source": [
    "Nun betrachten wir wieder das Test-Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "39Wfcg1xy27E"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_cm(y_true, y_pred, classes, title='Konfusionsmatrix', cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() * 2. / 3\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Tatsächlich')\n",
    "    plt.xlabel('Vorhersage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rman8Yre9JMo",
    "outputId": "e163807c-db66-407a-ad9b-92a7152ad14d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 1s\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x=test_batches, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktflor_V9JsA",
    "outputId": "4d0ed38b-2b84-44c1-da96-d1ba2bce471a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Actual:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "preds = np.argmax(predictions, axis=1)\n",
    "print(\"Predictions:\\n\", preds)\n",
    "print(\"Actual:\\n\", test_batches.classes)\n",
    "print(\"Accuracy:\", sum(preds == test_batches.classes)/preds.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "4yo9-6e1y3ml",
    "outputId": "13aea7e6-5d53-4e4e-bfa5-9620d33e411b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEmCAYAAAAA6gkZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeE0lEQVR4nO3debxWVb3H8c/3AAoKKKQozqkgohYKWipYSl7JIS2HnPE6Z2ZpWZZmVk5du2YOaGQajqWZgl4rDUMlERNFw6GLlkPJZXBIKFHA3/1j74MP+EwHn2E9nO+71349e3r2/p2D59dae629liICMzN7v7ZmB2BmlionSDOzEpwgzcxKcII0MyvBCdLMrAQnSDOzEpwgDUlbSHpc0nxJp3yA61wl6du1jC0FkhZI2rTZcVjjyf0gW4ukF4BjI+L3+fbBwJXAfhFx/wpe82fAmxFxas0CbQGSJgE3RMTVzY7F0uQSZAuTNBq4AthrRZNjbmPgqdpEtfKQ1LXZMVhzOUG2KEnHA/8N7BERD0laT9IESa9Jek7ScQXnniPpFknX5dXopyQNy4/dB+wKXJ5XJQdKmiTp2ILvHyVpcr4uST+SNEfSPyU9KWnr/NjPJZ1b8L3j8lhey2Nbr+BYSDpR0kxJr0u6QpLyY5tLuj+//jxJv1zueyfl35sv6fuSNpM0RdKb+c+5Sn5uH0l3SZqb3+MuSRvkx84DRhT83JcXXP+LkmYCMwv2bS5pFUnTJX0p399F0h8lnV3Df1pLSUR4aaEFeAG4DZgNfLRg//3AGKA7MASYC4zMj50DLAT2BLoAFwAPF3x3Elm1vdT2UcDkfH0PYBqwJiBgS6B/fuznwLn5+m7APGA7YFXgMuCBgmsGcFd+nY3yeEflx24GziT7P/DuwPDlvjcB6A1sBbwNTAQ2BdYAngZG5+d+CNgfWA3oBdwK3FHq5yy4/r1AX6BHwb7N8/Wtgdfzn/tM4GGgS7P/u/BSn8UlyNa0O9kf5p8BJG0IDAe+ERELI2I6cDVwRMF3JkfE3RGxBLge+OgK3nsRWbIZRPYM+5mImFXkvMOAayLisYh4G/gmsKOkTQrOuTAi3oiIl4A/kCX29ntsDKyX/zyTl7v2DyLizYh4CpgB3BMRf42IfwK/AbYFiIhXI+K2iPh3RMwHzgM+UcXPeEFEvBYRby1/ICJmAOcCtwNfA47If6e2EnKCbE0nAgOBq/Nq6XrAa3kSaPcisH7B9v8VrP8b6L4iz9gi4j7gcrJnn7MljZXUu8ip6+UxtH9vAfBqhZh65utfJyudPpI/Djh6uWvPLlh/q8h2TwBJq0n6iaQXJb0JPACsKalLhR/z5QrHxwGbAHdHxMwK51oLc4JsTXOAkWTP0MYArwB9JfUqOGcj4B8reP1/kVVL261beDAiLo2IoWRV3IHA6UWu8QpZKRAASauTVXkrxhQR/xcRx0XEesAJwBhJm3f4p4CvAlsAH4uI3sAu7eG036pUCBWuO4bs8cAekoavQFzWIpwgW1REvEL2nG8UcBrwEHCBpO6SPgIcA9y4gpefDnwuL4Ftnl8LAEnbS/qYpG5kiXQhUKyKeRPwn5KGSFoVOB+YGhEvVLq5pAPbG1PInvdFiXtU0ousRPmGpL7Ad5Y7Ppvs2WXVJB0BDCV7LnsKME5Sz7JfspblBNnCIuJlsiR5APAcWbXvFbLnY9+JiHtX8NI/At4hSyDjWDbR9gZ+Spa4XiSrNv+wSGwTgW+TNSjNAjYDDq7y/tsDUyUtIGuQ+XJE/G0Ffo5LgB5kjUUPA79d7viPgQPyFu5LK11M0kb5NY+MiAURcRPwKNnvy1ZC7ihuZlaCS5BmZiU4QZqZleAEaWZWghOkmVkJLf8yvrr2CK3Sq/KJlpxtt9yo2SHYCnrxxReYN2+eKp9ZvS69N45Y/L6Xl4qKt+b+LiJG1fL+xbR+glylF6tucVCzw7AV8Meplzc7BFtBO39sWM2vGYvfqvpveeH0K9aqeQBFtHyCNLOVhUBpPfVzgjSzNAhQTWvtH5gTpJmlo63SOCKN5QRpZolwFdvMrDRXsc3MihAuQZqZFSeXIM3MSnIJ0sysGLkV28ysKPeDNDMrw1VsM7Ni3A/SzKy0Nlexzczez/0gzczKcCONmVkx7uZjZlaaq9hmZkXIrxqamZXmEqSZWQkuQZqZFeOO4mZmxQm3YpuZFecSpJlZaX4GaWZWgkuQZmYluARpZlaE/AzSzKw0lyDNzN5PQFubS5BmZu+nfEmIE6SZJULIVWwzs+KcIM3MSnCCNDMrwQnSzKwIScizGpqZFecSpJlZCaklyLR6ZZpZpyapqqXKa3WR9Liku/LtvpLulTQz/+xT6RpOkGaWBnVgqc6XgWcKts8AJkbEAGBivl2WE6SZJaNWJUhJGwB7AVcX7N4XGJevjwP2q3QdP4M0sySoY2/SrCXp0YLtsRExtmD7EuDrQK+CfetExCyAiJglqV+lmzhBmlkyOtDNZ15EDCt6DWlvYE5ETJP0yQ8SjxOkmaVBNWvF3hn4jKQ9ge5Ab0k3ALMl9c9Lj/2BOZUu5GeQZpaMWjyDjIhvRsQGEbEJcDBwX0QcDkwARuenjQbGV4rHJUgzS0ad+0FeCNwi6RjgJeDASl9wgjSzJHSwkaYqETEJmJSvvwqM7Mj3nSDNLB1pvUjjZ5CpaWsTU27+Brf9+EQAthm4PpPGfZU/3fItfnXJCfRavXuTI7RKTjj2aDZarx9Dh2zd7FBai7IpF6pZGsUJMjEnH7orf/nb7KXbV559KGddOp7tDzqfCX94glNHd6iGYE1wxOijGH/Xb5sdRkuq5auGteAEmZD1+63JqOFbce3tDy3dN2Djfkye9hwA9z38LPuNHNKk6Kxaw0fsQt++fZsdRmuq7auGH5gTZEIuOn1/zvzxHbz7bizd9/Tzs9j7k9sA8Lndt2ODdSq+X2/WsjptCVLSupJ+Iel5SU9LulvSwBLnrinppEbFloJPj9iaOa/N5/FnXl5m/wnn3MgJB+3CH2/8Oj1XW5V3Fi1pUoRm9VVtcmxkgmxIK7ayn+h2YFxEHJzvGwKsA/xvka+sCZwEjGlEfCnYccim7P2JbRg1fCtWXaUbvVfvzjXnHsnRZ13HPiddAcDmG/Xj0yO2anKkZvWT2niQjermsyuwKCKuat8REdMl9ZQ0EegDdAPOiojxZB06N5M0Hbg3Ik5vUJxNc/ZlEzj7sgkAjBg6gK8cOZKjz7qOtfv0ZO7rC5DEGcftwU9/NbnJkZrVT2dNkFsD04rsXwh8NiLelLQW8LCkCWTjtG0dEUOKXUzS8cDxAHTrWZeAU3HQqGGc8PldABh/33SuG/9wkyOySo48/BAevH8S8+bNY7NNNuDbZ3+Xo44+ptlhtQTPSbMsAedL2gV4F1ifrNpdVj6s0ViAttX6RYXTW86D02by4LSZAFxx8ySuuHlScwOyDrnuhpubHUJrqt1gFTXTqAT5FHBAkf2HAWsDQyNikaQXyEbfMLNORkBi+bFhrdj3AatKOq59h6TtgY3Jxm1bJGnXfBtgPssOdGlmK730WrEbkiAjIoDPArvn3XyeAs4B7gaG5SMDHwY8m5//KvBHSTMkXdSIGM2s+aTqlkZp2DPIiHgFOKjIoR1LnH9ofSMys9R01meQZmZlSdClixOkmVlRiRUgnSDNLB2uYpuZFdPgBphqOEGaWRKyfpBpZUgnSDNLRGP7OFbDCdLMkpFYfnSCNLNEKJuTKSVOkGaWBD+DNDMrI7H86ARpZulwCdLMrITE8qMTpJklohMPmGtmVlaKA+Y6QZpZIuRuPmZmpbiKbWZWjAerMDMrzh3FzczKcII0MyshsfzoBGlmifBgFWZmxcnjQZqZlZZYfqSt2QGYmbVrk6paKpHUXdIjkp6Q9JSk7+b7+0q6V9LM/LNPuetUVYKUtBOwSeH5EXFdNd81M6tWDUuQbwO7RcQCSd2AyZJ+A3wOmBgRF0o6AzgD+Eapi1RMkJKuBzYDpgNL8t0BOEGaWc2ohoNVREQAC/LNbvkSwL7AJ/P944BJfJAECQwDBuc3NDOrmw40Yq8l6dGC7bERMbbwBEldgGnA5sAVETFV0joRMQsgImZJ6lfuJtUkyBnAusCsqkM3M1sBHejmMy8ihpU7ISKWAEMkrQncLmnrjsZTMkFKupOsSNoLeFrSI2T1+vabf6ajNzMzK0VkXX1qLSLekDQJGAXMltQ/Lz32B+aU+265EuQPaxijmVlFteonLmltYFGeHHsAnwJ+AEwARgMX5p/jy12nZIKMiPvzG30YmBURC/PtHsA6tfghzMyWUk07ivcHxuXPIduAWyLiLklTgFskHQO8BBxY7iLVPIO8FdipYHtJvm/7FQrbzKyEWuXHiHgS2LbI/leBkdVep5oE2TUi3im4wTuSVqn2BmZm1RBU1Qm8kap5k2aupKUNMpL2BebVLyQz66za2lTV0ijVlCBPBG6UdDlZkn8ZOLKuUZlZp6NWHFE8Ip4HPi6pJ6CImF//sMysM0qtil2uH+ThEXGDpNOW2w9ARFxc59jMrJNJKz2WL0Gunn/2akQgZmYtMx5kRPwk//xu48Ixs84qa8VudhTLKlfFvrTcFyPilNqHY2adVm07itdEuSr2tIZFYWZGC81JExHjGhmImXVuLVXFbidpIPA13j+i+G71C8vMOqNWqmK3uxW4Cria90YUNzOrubTSY3UJcnFEXFn3SMysU5Naq6N433z1TkknAbez7IC5r9U5NjPrZBLLjxVbsYP3Sr2nFxwLYNN6BWVmnVMrtWJ/uJGBmFnnJqqb87qRqmnF/iJwY0S8kW/3AQ6JiDF1jq0qQ7bciAceKtun3RLVZ6evNjsEW0FvP/v32l80wdF8qhkP8rj25AgQEa8Dx9UtIjPrtJS/TVNpaZRqWrHbJKl9Xux8jgePKG5mNVdNia2RqkmQvyOb5OYqssaZE4Hf1jUqM+t0RGt2FP8GcALwBbKf4R6yTuNmZjWVWCN2VSOKvwtcmS9mZnUhQZfEMmQ1rdgDgAuAwUD39v0R4X6QZlZTieXH0s9EJU3OV68lKz0uBnYFrgOur39oZtbZtE/cVWlplHKNRnvmnz0iYiLZhF0vRsQ5gEfyMbOaap8Xu5qlUcolyJvyz7eVNS3NlHSypM8C/eofmpl1Nm1VLo2Mp6iI2DtfPRXoCZwCDAWOAEbXPzQz62xSq2JX04o9NV+dL+kYoGdEvFnfsMyss5GUXCt2xdKqpJsk9Za0OvA08BdJp1f6nplZR7WpuqVh8VRxzuC8xLgfcDewEVk128ysZlqtkaZdN0ndyBLk+IhYVN+QzKyzSu0ZZDUJ8ifAC8DqwAOSNgb+Wc+gzKwTqrJ63cgqdjXvYt8ZEUsHXJT0EnB0/UIys85KiU3bVU0J8rbCjXzYs1/UJxwz66za58VuiRKkpEHAVsAakj5XcKg3Be9km5nVSmrdfMpVsbcA9gbWBPYp2D8fjyhuZjXWXoJMSblJu8YD4yXtGBFTGhiTmXVGCc5JU00jzeP5xF1bsexwZ26oMbOaqlUfR0kbko08ti7wLjA2In4sqS/wS2ATst45B+XzbBWPp4p7XZ/fZA/gfmADsmq2mVnN1LiRZjHw1YjYEvg48EVJg4EzgIkRMQCYmG+XVG48yPbS5eYR8W3gXxExDtgL2KaqEM3MOqBWHcUjYlZEPJavzweeAdYH9gXG5aeNI3sBpqRyJchH8s/2N2fekLQ1sAZZ8dTMrIZEW5VLh64qbQJsC0wF1omIWZAlUSoM3VjNM8ixkvoAZwETyIY++3aHIjQzqyCbk6bq09eS9GjB9tiIGPv+a6onWV/ur0TEmx2dNbFcguwn6bR8/T/zzyvyz9U7dBczsyp0oJFmXkQMK3dCPobEbcCNEfHrfPdsSf0jYpak/sCcsvGUOdaFrLTYq2DpWbCYmdVMNi92bZ5B5rMg/Ax4JiIuLjg0gfcG/B4NjC93nXIlyFkR8b3KoZiZ1UYNhzLbmWxYxj9Lmp7v+xZwIXBLPvj3S8CB5S5SLkEm1mXTzFZ2tcqPETGZ0jlsZLXXKZcgq76ImdkHJRo7IVc1yr1q+FojAzGzTk41rWLXRDXdfMzM6q59yoWUOEGaWTLSSo9OkGaWkMQKkE6QZpYK0dE3XerNCdLMktBSrdhmZo3mEqSZWTHu5mNmVpyr2GZmZbiKbWZWQlrp0QnSzBKSWAHSCdLM0pA9g0wrQzpBmlki5FZsM7NSEsuPTpBmlgZXsc3MSqlyvplGcoI0s2Q4QZqZlaDEqtipvdljwN9ffpk9/2MkQz+6Fdtvuw1jLr+02SFZFdraxJTrT+O2i48B4CMD1uP+n53CwzecxuRxX2HY4A2bHGHashHFq1saxSXIBHXt2pXzf3ARQ7bdjvnz5zNix+3ZbeSnGLTl4GaHZmWcfPAI/vLCbHqt3h2A8760N+ddfQ/3THmWPXYaxHlf2ps9vnBlk6NMW2rdfFyCTNC6/fszZNvtAOjVqxdbDBrEK//4R5OjsnLW77cGo3YezLXjpy7dF0DvPFmu0bMHs+a92aToWoeq/F+juASZuBdfeIEnp09n2A4fa3YoVsZFp+7LmZfdRc/VVl267/SL7+DOS4/ngi/vQ5vErsde1sQI09dexU5J3UqQkpZImi7pKUlPSDpNkkusHbBgwQIOP+RALvzhxfTu3bvZ4VgJnx6+JXNeX8Djz/59mf3H778TX//ReAbs832+fsl4rjzroCZF2CqqLT+uHCXItyJiCICkfsBNwBrAd+p4z5XGokWLOPzgAzjo4EPZd7/PNTscK2PHj3yYvUdsxaidtmTVVbvSe/XuXPPdQ9lzxGC++t93AHDb759gzLecIMtKsB9kQ0p0ETEHOB44WZnukq6V9GdJj0vaFUDSapJukfSkpF9KmippWCNiTElE8MUTjmWLQVvypS+f2uxwrIKzx9zN5vt8n0H7nceRZ97ApEef4+jv3MSsuW8yYrvNAPjk9gN47uW5TY40fapyaZSGPYOMiL/mVex+wOH5vm0kDQLukTQQOAl4PSI+ImlrYHqxa0k6nizhsuGGGzUi/Iaa8tAfufmmG9hq623YaYesseY73zuXPUbt2eTIrCO+eP6tXHTavnTt2oW3317EyRf8qtkhJU1Al8SKkI1upGn/6YcDlwFExLOSXgQG5vt/nO+fIenJYheJiLHAWIDthg6LegfdaDvtPJz5C5c0OwxbAQ8+9jwPPvY8AA898Td2Hn1JcwNqNWnlx8YlSEmbAkuAOZT+NST26zGzRuqUb9JIWhu4Crg8IgJ4ADgsPzYQ2Aj4CzAZOCjfPxjYphHxmVkapOqWRqlnCbKHpOlAN2AxcD1wcX5sDHCVpD/nx46KiLcljQHG5VXrx4EngX/WMUYzS0ha5cc6JsiI6FLm2ELgqCKHFgKHR8RCSZsBE4EX6xOhmSUnsQyZ2ps0qwF/kNSN7Ff1hYh4p8kxmVkDZF140sqQSSXIiJgPdLp+j2YGNHiknmoklSDNrJNzgjQzK6ax71lXwwnSzJKR2Is0Hg/SzNJQ7XvY1eRQSddImiNpRsG+vpLulTQz/+xT6TpOkGaWjtqNVvFzYNRy+84AJkbEALIuhGdUuogTpJklo02qaqkkIh4AXltu977AuHx9HLBfpev4GaSZJaMDjyDXkvRowfbYfBCbctaJiFkAETErH6e2LCdIM0tDxwZ7nBcRde8z7Sq2mSWjzlMuzJbUHyD/nFPpC06QZpYEUffRfCYAo/P10cD4Sl9wgjSzZNSwm8/NwBRgC0l/l3QMcCGwu6SZwO75dll+Bmlm6ahRR/GIOKTEoZEduY4TpJklo5ouPI3kBGlmyUgrPTpBmllKEsuQTpBmlgQPmGtmVkqDJ+SqhhOkmSUjsfzoBGlmqRBKrAjpBGlmyUgsPzpBmlkaOjZWRWM4QZpZOhLLkE6QZpYMd/MxMyvBzyDNzEpILD86QZpZIoS7+ZiZFdM+YG5KnCDNLBmJ5UcnSDNLh0uQZmYluJuPmVkpaeVHJ0gzS0di+dEJ0szSIHlOGjOz0tLKj06QZpaOxPKjE6SZpSOxGrYTpJmlQu7mY2ZWjF81NDMrwwnSzKwEV7HNzIrxvNhmZsV50i4zs3ISy5BOkGaWDD+DNDMrwc8gzcxKcII0MyshtSq2IqLZMXwgkuYCLzY7jjpaC5jX7CBshazM/3YbR8TatbygpN+S/c6qMS8iRtXy/sW0fIJc2Ul6NCKGNTsO6zj/27W+tmYHYGaWKidIM7MSnCDTN7bZAdgK879di/MzSDOzElyCNDMrwQnSzKwEJ0gzsxKcIM3MSnCCTJykHgXrPZsZi1ln41bshOXJ8WjgMaA/MBi4MCIWNzUwq5okRURI6gP8KyLeaXZMVj0PVpGwiHhL0mPAXcAbwKCIWNz+R9fc6KySguS4A3AucBlwZ5PDsg5wFTtB0jKDPs0FngbeAUZUONcSkifHUcDXyP7WrpC0h6QuTQ7NquQEmZjC0qGk1SPiuYgYAZwCXCJp//wPb0dJ/VySTJek9YDvAxdHxKeAC4CzgLqPQmO14QSZmILk+DXgWkkPSNohIu4FzgZ+KOkq4CL8iCR1c4CZQBeAiLgSmAJclVe7XQNInBNkIiQNlbSDpO6STgA+DRwCBHCrpP+IiDuAw4B/A8dGxCvNi9iW157sJK0h6UN5Y9os4GN5aRLgFrKkebWkNV0DSJtLIAmQtBdwPnAx8CrZv8tRwFeA2cAvgV9IGh0Rd0p6OCLebVK4VkL+6OMzwOlAm6R7gAnAqcDmkhaRPUc+EDgDWIes8c0S5W4+TSbpE8DPgMMiYmrB/o2Ba4HPRMQCSQ+RlRz3iYi3mhOtFVPQWj0YGAccT1a9vpSsSn0TsB1ZN627gH7AlcBuETGrOVFbNVyCbL6hwGURMVVSt4hYlO+fC/wd2F9SAE8CFzg5pqOgQU1kj0JWAf4B/CUi/i3pWGAyMCcirgPukvRx4KfAZ50c0+cE2SQFf1wfBv6Z7y7sAL6YLCkOBz4OfD4iVua5d1qKpIHAEZLWALpI+i/gJbJS/kclPRkRr0u6YrmvPkpWcny5wSHbCnAjTZMUPJy/Hfi4pKF5Na1NUpf8jYsALgc+ERFPNy1YW4akLYBfA68BzwPvAg8B6wP3kfV7/IKkI4HTgJfz7ykiFjs5tg6XIJtvKlk17POSiIhpAJIOBg4FfhkRrzUzQHtP/pzxRuBbETGhYP9ssrdkhpLNsrkDsBtwUkT8AZb5P0VrEW6kSYCk9YFjgJHAn4CFwAHAARExo5mx2bIkDQceiIi2fLtH+3NhSZcAa0bEUfl24TNla0GuYicgIv5B1vH7TGABWZXsM06O6YmIycBekp7P+zq+Jal7fngK0K3gdA8q0uJcxU5EXgqZnC+WsIj4jaSTgUckbV/wCORt4A1J3YDFrlK3PpcgzVZARPwGOJmsVbq94eZC4M6IWOTkuHLwM0izD0DSp4HbgL8Bp0fE3U0OyWrICdLsA5I0EugdEbc3OxarLSdIsxrxQMYrHydIM7MS3EhjZlaCE6SZWQlOkGZmJThB2lKSJknaY7l9X5E0psrvL6hPZGbN4QRphW4GDl5u38H5/rI+6Ex9kvxWlyXHCdIK/QrYW9KqAJI2AdYDNpD0Z0kzJP2g/WRJCyR9T9JUYMd833mSnpD0sKR18n1rS7pN0p/yZed8/zmSxuZTE1wnaStJj0iaLulJSQPy8+6QNE3SU5KOL7j/MZL+Ny/5/lTS5eXuZ9ZhEeHFy9IF+B9g33z9DLLRr18C1iZ7d/8+YL/8eAAHFXw3yKaEAPgv4Kx8/SZgeL6+EfBMvn4OMA3okW9fRjb1BGSjc7fv75t/9gBmAB8iS9wvAH3JBoh4ELi83P28eOno4mqNLa+9mj0+/7wdmBQRcwEk3QjsAtwBLCF7za7dO2RzrkCW+HbP1z8FDC6Y4bS3pF75+oR4bxqJKcCZkjYAfh0RM/P9p0j6bL6+ITAAWBe4P/KBIiTdCgwsd7+ImN/xX4d1Zk6Qtrw7gIslbUdWYnsC2KzEuQsjYknBduEgDUt477+vNmDHWG4+nTyB/at9OyJuyqvrewG/y+d0eZcs4e0Y2Twvk4DuZPPAlFL0fmYd5WeQtoyIWABMAq4hK01OBT4haa28IeYQ4P4OXvYespFvAJA0pNhJkjYF/hoRl5JNl/oRYA3g9Tw5DiKbnwfgkTyuPnkDz/4dvZ9ZJU6QVszNwEeBX0Q28943gT+QlSYfi4jxHbzeKcCwvOHlaeDEEud9HpghaTowCLgO+C3QVdKTwPeBh2HpIMPnkyXw3wNP897kZ9Xez6wsv4ttLUtSz8jmDO9K9qz0mvCIOlZDLkFaKzsnL23OIBuP8Y6mRmMrHZcgzcxKcAnSzKwEJ0gzsxKcIM3MSnCCNDMrwQnSzKyE/wfi8UkBuDa+bgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(test_batches.classes, preds, classes)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "transfer_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
